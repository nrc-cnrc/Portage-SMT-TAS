Technologies langagieres interactives / Interactive Language Technologies
Institut de technologie de l'information / Institute for Information Technology
Conseil national de recherches Canada / National Research Council Canada
Copyright 2004-2010, Sa Majeste la Reine du Chef du Canada
Copyright 2004-2010, Her Majesty in Right of Canada

Distributed under specific licensing terms.  Please refer to your signed
license agreement for details.


PORTAGEshared note
------------------

This rescoring example was included as part of an earlier distribution which
only included rescoring software.  It is kept here since it may still be
informative, but it is superceeded by the much more complete rescoring suite
included with PORTAGEshared.


NRC Rescoring Toolkit
---------------------

This is a set of Linux binary programs to perform nbest rescoring.

Given a source text, a set of "n" alternate translations (ie an nbest list) for
each source sentence, and one or more reference translations, the aim is to
learn a loglinear model that can be used to pick the best translation for each
source sentence from the corresponding nbest list.

The model takes as input a vector of feature values that characterize a
particular translation hypothesis, and produces a score that indicates how good
the hypothesis is. The highest-scoring hypothesis in each nbest list is the one
that is chosen. Feature values are real numbers that describe various salient
attributes of the hypothesis, such as the average value of the ascii codes of
the letters it contains. All features must be supplied by you - they are not
part of the toolkit.

Given a vector v of feature values for some hypothesis, the model's score is
exp(v . w), where w is a set of weights on features, and the "." is a dot
product. The w are the parameters of the model, which we aim to set so as to
maximize the BLEU score of the resulting translations. Since BLEU is a
non-differentiable function of w, this is a nasty optimization problem.  The
algorithm used to solve it is described in detail in the article "Minimum Error
Rate Training in Statistical Machine Translation", Franz Josef Och, ACL
2003. It works well for fairly small feature sets.

Here are the programs included in the toolkit (in the "bin" directory):

rescore_train      - Train a rescoring model.
rescore_translate  - Translate with a rescoring model.
bleumain           - Calculate BLEU score for a translation, given references.
bestbleu           - Estimate best possible BLEU score for an nbest list.

All programs can be run with -h to give usage information.

The main steps in using the toolkit are training models and evaluating their
performance on new text. An optional preliminary step is to choose the nbest
list size by estimating the best BLEU score obtainable for a given size when
reference translations are known (since BLEU is not additive across different
source sentences, this is not quite as straightforward as it may seem). A
non-optional, even more preliminary step is to generate nbest lists and
corresponding feature values, but, as mentioned above, this is not handled by
the toolkit.

Examples
--------

Here are some examples to illustrate the use of the toolkit. They refer to the
sample files contained in the "example" directory, which include a small
development set for training the model and a test set for evaluating it. The
sets are for Chinese to English translation, and each contain: a source file
(.src), four reference translations (.ref*), a 1000-best file (.nbest), a set
of basic decoder features (.ffvals), and two IBM-model-based features (.ibm-*).
All text files contain one sentence per line; the nbest lists for successive
source sentences are concatenated together. Feature files correspond line for
line with nbest files.

The files in the "standard" subdirectory should match files generated by the
examples.

1. Calculating oracle BLEU scores

This command calculates the estimated best BLEU score on the dev set for
10-best, 100-best, and 1000-best lists:

   bestbleu -n 10 -n 100 -n 1000 dev.nbest dev.ref*

Here is a summary of the output that shows how BLEU score increases with
nbest size:

10   BLEU score: 0.321263
100  BLEU score: 0.358523
1000 BLEU score: 0.393848

Based on this, it would probably be worthwhile to increase nbest size by
another order of magnitude (but not if we want to have examples run fast!).

2. Training a model

The first step in training is to create a rescoring model that specifies the
features to use. All your features must be pre-calculated and stored in
external feature files. Examples of these are given in the sample model file
"model.in". Eg, the line "FileFF:ffvals,3" means take values from the 3rd
column in the file "ffvals".

Here is a typical training command, using "model.in":

   rescore_train -vn -p dev. model.in model.out dev.src dev.nbest dev.ref*

where "model.out" is a version of "model.in" that will contain the learned
weights.  The output from rescore_train indicates that the BLEU score on the
dev set for this model should be 0.312429. As a sanity check, this can be
verified using the evaluation program described below.

The use of the -p switch here is slightly subtle. Notice that, although the
FileFF features in "model.in" refer to the file "ffvals", there is no such
file. -p causes a string, in this case "dev.", to be prefixed to all FileFF
arguments. The point of this is to allow a single model to work with feature
files generated for different source files, which must be given different
names if they are to coexist in the same directory.

3. Translating and testing.

Translating with a trained model is straightforward:

   rescore_translate -p test. model.out test.src test.nbest > test.out

If reference translations are available, the BLEU score for this translation
can be calculated using bleumain:

   bleumain test.out test.ref* > test.bleu

In this case, the score is 0.269352, which is quite low. The small size of the
dev set has clearly caused overfitting.
