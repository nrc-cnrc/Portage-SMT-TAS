Traitement multilingue de textes / Multilingual Text Processing
Centre de recherche en technologies num√©riques / Digital Technologies Research Centre
Conseil national de recherches Canada / National Research Council Canada
Copyright 2008-2018, Sa Majeste la Reine du Chef du Canada
Copyright 2008-2018, Her Majesty in Right of Canada


                            PortageII unit-testing

This directory contains a collection of ad hoc unit test suites.  These suites
typically take too long or are too complex to integrate them into the unit
testing framework we use in the src/ subdirectory.  Some of these test suites
are self verifying and automated via a run-test.sh script.  These get exercised
if you run ./run-all-tests.sh.  Others are only partially automated and require
manual analysis.

After installing PortageII, you can run the automated test suites by invoking
./run-all-tests.sh.  If all tests pass, great.  If not, differences in floating
point rounding between your architecture and ours is a likely cause, and might
not indicate an real problem.  In particular, these test suites are known to be
especially sensitive to floating point variance: align.posteriors, joint2cond,
hmm-*, rescoring.metric. 

Contents of this directory
==========================

Scripts:
 - run-all-tests.sh       Master script running all fully automated suites.
 - clean-all-tests.sh     Clean up after running test suites.

Test suites:
 - add-fr-nbsp/           Test add-fr-nbsp.pl
 - al-diff.py/            Test al-diff.py
 - align.posteriors/      Test suite for the PosteriorAligner.
 - arpalm2tplm.sh/        Test building TPLMs
 - bilm/                  Test the bilm features
 - build-tp-suffix-array.sh/ Test building tightly packed suffix arrays
 - canoe.compress.output/ Make sure different ways of creating n-best produce
                          the same results.
 - canoe-daemon/          Test the canoe daemon feature and per-sentence load b.
 - canoe-misc/            Tests to fix misc problems with canoe
 - canoe_newSrcSentInfo2JSON/ Test that canoe can output its NSSI to JSON
 - canoe_TriangularArray/ Test that canoe can write a triangular array per sent
 - cat.sh/                Test parallel IBM model training
 - ce_tmx.pl/             Test processing TMX and sdlxliff files
 - check-installation/    Check that installation of prerequisites and
                          PortageII is OK
 - chinese_rule_markup.pl/ Test chinese_rule_markup.pl
 - coarse-lm/             Test coarse LMs
 - coarse-tm/             Test coarse TMs
 - compile_truecase_map/  Test suite for compile_truecase_map.
 - configtool/            Test suite for configtool.
 - count_unal_words/      Test suite for count_unal_words
 - crontab/               Test the recommended incremental cleanup crontab
 - dist-limit/            Test various distortion limit implementations
 - distortion-limit/      Check that canoe's -dist-limit-{ext,swap,simple} work
 - diversity/             Test canoe's -diversity, -diversity-stack-increment,
                          -cov-limit options work
 - dynamic-pt/            Test suite for combining phrase tables
 - dynmap.number/         Test DynMap LMs
 - fast_align_normalize_ttable.py/ Test fast_align_normalize_ttable.py
 - filter-distortion-model/ Test suite for filter-distortion-model.
 - filter-long-lines.pl/  Test our long-line filter
 - filter_models/         Test suite for filter_models.
 - fixed_term2tm.pl/      Test compiling a list of fixed terms for use
 - fix-en-fr-numbers/     Test rule-based handling of numbers for en<->fr
 - forced-decoding/       Test canoe -forced
 - gen_feature_values/    Test generating rescoring features
 - gen-jpt-parallel.sh/   Test generating rescoring features in parallel
 - gen_phrase_tables/     Test generating phrase tables
 - giza_filter/           Test our filter to prepare input for giza
 - hmm-classes/           Test suite for the class-conditioned variant of the HMM
                          word alignment module.
 - hmm-lex/               Test suite for the lexically conditioned variant of the
                          HMM word alignment module.
 - hmm-sym/               Test suite for symmetrized training of HMM word
                          alignment models.
 - ibmcat/                Test the multi-reader of all IBM files
 - ibm-och-aligner/       Test align-words's IBMOchAligner algorithm
 - IdentityAligner/       Test align-words's IdentityAligner algorithm
 - incr-add-sentence/     Test suite for incr-add-sentence.sh
 - incr-init-model/       Test suite for incr-init-model.py
 - incr-update/           Test suite for incr-update.sh
 - joint2cond/            Test suite for joint2cond_phrase_tables, including its
                          -reduce-mem and -prune1 options.
 - joint2multi_cpt/
 - lm/                    Test suite for using our various LM file formats and
                          filtering options.
 - lm-context/            Test canoe's -minimize-lm-context-size
 - lm-filter.py/          Test filtering LM in ARPALM format
 - lm-order-error/        Test using arpalm2binlm | binlm2arpalm to patch bad
                          ARPALM files
 - lm.train_mixture/      Test suite for train_lm_mixture.
 - madamira.py/           Test suite for our MADAMIRA wrapper (skipped if MADAMIRA not found)
 - markup_canoe_output/   Test suite for the tools of tags transfer from source
                          to target text.
 - merge_counts/          Test suite for merge_counts and its uses for parallel
                          counting of phrase tables and LMs.
 - merge_multi_column_counts/ Test merge_multi_column_counts
 - merge_sigprune_counts.py/ Test the C++ implementation of merge_sigprune_counts
 - nbest2rescore.pl/      Test suite for all outputs from nbest2rescore.pl
 - nnjm/                  Test using NNJMs in canoe
 - nnjm-genex.py/         Test suite for nnjm-genex.py
 - nnjm.reader/           Test suite to validate a NNJM model format
 - parallelize.pl/        Test suite for parallelize.pl and its funky uses
 - parse_tokan.pl/        Test parsing MADA+TOKAN output
 - perl_interpreter/      Test the syntax to get the user Perl, not system one
 - PortageLiveAPI/        Test the PortageLive SOAP API (not run automatically)
 - PortageLiveLib/        Test suite for PortageLiveLib
 - PortageLive.rest/      Test suite for the PortageLive REST API
 - PortageLive.soap/      Test suite for the PortageLive SOAP API
 - portage_utils.pm/      Test suite for Perl magic streams
 - portage_utils.py/      Test Python magic streams and other Python utils
 - predecode_plugin/      Test the components of the predecode plugin
 - prime.sh/              Make sure prime.sh works for PortageLive systems
 - rescoring.metric/      Test suite for using PER or WER in cow.sh.
 - rest-cost-lm/          Test RestCostLM training and use
 - run-parallel.io/       Test STDERR and STDOUT capturing in run-parallel.sh.
 - run-parallel.local-workers/ Test the choosing of the number of workers in
                          the master job under varying circumstances.
 - run-parallel.RP_PSUB_OPTS/ Make sure RP_PSUB_OPTS=... works correctly.
 - r-parallel-d.pl/       Test suite for the run-parallel deamon's ability to
                          detect when its parent has disappeared and then exit.
 - script.ff/             Test suite for the SCRIPT: rescoring feature.
 - second-to-hms.pl/      Test converting seconds to HM:MM:SS and back
 - sigprune.sh/           Significance pruning test suite
 - sparse/                Test SparseModel training and use in decoding
 - stripe.py/             Test performing striped splitting
 - strip_non_printing/    Test suite for strip_non_printing.
 - textpt2tppt.sh/        Test creating TPPTs
 - time-mem/              Test time-mem
 - tm-count/              Test handling counts in phrase tables
 - tmx2lfl.pl/            Test extracting text from TMX files
 - tmx-prepro/            Test suite for the basic functionality of tmx-prepro
 - tokenize.pl/           Test suite for recent enhancements to our tokenizer.
 - tokenize_plugin/       Test tokenizing Arabic (requires MADA)
 - tokenize_plugin_ar/    Also test tokenizing Arabic (requires MADA)
 - toy/                   *** OBSOLETE *** Complete use of the PORTAGEshared 1.0
                          through 1.2 software suite on a tiny corpus, with
                          parameters chosen to run very fast (*not* recommended
                          as an experimental starting point).
 - translate.pl/          Test suite for doing the whole translation pipeline
 - truecasing/            Test suite for truecasing.
 - tp-alignment/          Test the tightly-packed alignment file format
 - tpldm/                 Test TPLDMs (tightly-packed lexical distortion models)
 - tppt/                  Test TPPTs
 - train_tm_mixture/      Test making mixture TMs
 - tune.py/               Test suite for decoder weight tuning
 - udetokenize.pl/        Test detokenization with -paraline and -deparaline
 - unal/                  Test suite for decoder features counting unaligned words
 - walls-zones/           Test canoe's Walls and Zones features
 - wc_stats/              Test suite for our enhanced word counter
 - word2class/            Test suite for word2class
 - word_align_tool/       Test suite for word_align_tool.
 - zensprune/             Test Zens pruning (based on compositionality scores)


Types of unit tests in PortageII
================================

There are two types of unit tests in PortageII:

 - The CxxTest tool is used to create closely integrated unit tests, calling
   individual functions, linked with your code directly.  These are found in
   the src/ hierarchy, under tests/ subdirectories and are exercised when you
   run "make test" in the PortageII/src/ directory.  See the annotated example
   in src/utils/tests/test_your_class.h for further details.

 - This directory is used to create loosely integrated unit tests, calling your
   actual programs via scripts and/or Makefiles.


Running the unit tests
======================

The unit tests in this directory are exercised by the nightly autobuild, and
can be launched manually this way:
   ./run-all-tests.sh -j 30
This will launch all tests, 30-ways parallel, and report on the success or
failure of each test.

On a non-clustered machine, you probably want to use -j with a third to half
the number of hyperthreaded cores on the machine (e.g., -j 12 to 15 on a
machine with 16 cores hyperthreaded to 32 apparent cores).

To clean up all tests, you can run
   ./clean-all-tests.sh


Minimum requirements
====================

In order for a unit test to work properly in this directory, you should:

1. Create an executable file called run-test.sh which runs the test and exits
with a 0 status on success, non-0 status on failure.  Doing so will make
./run-all-tests.sh run your test and report on its success or failure.

If the test needs to be skipped because it relies on some optional dependency
that is missing, it should exit with status 3.  Note: don't use this status for
tests that rely on mandatory Portage dependencies, only tests that rely on
optional Portage dependencies.  See bilm/run-test.sh, for example.

2. Create a Makefile that contains at least this code:
   TEMP_FILES=<list of temp files that might be created when the test is run>
   TEMP_DIRS=<list of temp directories that might be created when the test is run>
   include ../Makefile.incl
Doing so will give you a "clean" target so that "make clean" will remove all
temporary files, and it will give you a "gitignore" target that is
automatically called by "make all" so that Git will not see these temporary
files as untracked.  You can omit TEST_FILES or TEMP_DIRS if you don't create
temporary files or directories, but you should still include ../Makefile.incl
in all cases.

3. Write some unit tests.

If you write your unit test in the Makefile, make sure the "all" target runs
them. A typical run-test.sh file might then simply say
   make clean
   make all

If you write your unit tests in bash, your run-test.sh file should first say
   make gitignore
and then proceed to run your tests.


Parallelism guidelines
======================

It is strongly recommended to make each test suite finish in just a few
seconds, if possible.

A quick test suite should not be parallel (use "make all" without -j in
run-test.sh).

A slow test should use some parallelism, with the level of parallelism
chosen to minimize the total run time of
   ./run-all-tests.sh -j N
with N between a third and half the number of hyperthreaded cores on a
non-clustered machine.

A slow test can also detect when it's running on a cluster and maximize
parallelism in that case. See tune.py/run-test.sh for an example.


Additional suggestions
======================

Many test suites put their input data in a subdirectory called src/, and their
reference output in ref/.

The typical way to check for correctness is to manually inspect the output
while you create the test suite, save it in ref/ once you're happy with it, and
use diff for automatic subsequent validation.

