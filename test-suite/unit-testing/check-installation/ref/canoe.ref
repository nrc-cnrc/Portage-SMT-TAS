
canoe, NRC-CNRC, (c) 2004 - 2010, Her Majesty in Right of Canada
Please run "portage_info -notice" for Copyright notices of 3rd party libraries.

Usage: canoe -f CONFIG [options] < marked_up_src

Translate text, reading from standard input and writing to standard output.
Translation is controlled by the options listed below, which may be specified
in the CONFIG file or as command-line switches (the latter take precedence).

The CONFIG file contains a space-separated list of options, with each option
followed by an argument. Options are surrounded by square brackets:
'[option] arg' in CONFIG is the same as '-option arg' on the command line.
Arguments in CONFIG may be zero or more strings separated by whitespace or by
colons. The special sequence '--' is used to indicate an empty string. Any part
of a line that begins with a hash mark (#) is interpreted as a comment.

The input source text must be in marked-up format: \, < and > are special
characters and must be escaped with \ to be interpreted literally.  See the
user manual for a full description of the markup language.

Options (in command-line format):

 -ttable-file-s2t FILE1[:FILE2[:..]]
 -ttable-file FILE1[:FILE2[:..]]
 -ttable-file-f2n FILE1[:FILE2[:..]]
     The phrase translation model file(s) in text format containing backward
     probabilities.  That is, the left column of each table should contain
     source phrases and the probabilities should be p(source|target).

 -ttable-file-t2s FILE1[:FILE2[:..]]
 -ttable-file-n2f FILE1[:FILE2[:..]]
     The phrase translation model file(s) in text format containing forward
     probabilities.  That is, the left column of each table should contain
     target phrases and the probabilities should be p(target|source).
     Used for translation table pruning if enabled, and as a feature if
     -weight-f is specified.  If this option is used, the number of forward
     translation model files must match the number of backward translation
     model files.

 -ttable-multi-prob FILE1[:FILE2[:..]]
     The phrase translation model file(s) in multi-prob text format.  A
     multi-prob table combines the information of multiple forward/backward
     ttable pairs, replacing one or more -ttable-file-s2t/-t2s arguments
     pairs.  Each table must contain an even number of probability columns,
     separated by spaces, starting with all the backward probabilities,
     followed, in the same order, by all the forward probabilities.

     For example, if you combine a phrase probability estimate P_1 and a
     lexical probability estimate P_2 and other estimates up to P_n into a
     multi prob phrase table, each line would look like:
     s ||| t ||| P_1(s|t) P_2(s|t) .. P_n(s|t) P_1(t|s) P_2(t|s) .. P_n(t|s)
     where s is the source phrase and t is the target phrase.

     A multi-prob phrase table containing 2*N prob columns requires N
     backward weigths and, if they are supplied, N forward weights.

     A multi-prob phrase table may also contain a "4th column" with
     adirectional features, whose weights are supplied via -atm.  Assuming
     directional models P_1 and P_2 and adirectional score A_1 .. A_m,
     the syntax is:
     s ||| t ||| P_1(s|t) P_2(s|t) P_1(t|s) P_2(t|s) ||| A_1(s,t) .. A_m(s,t)

 -ttable-tppt FILE1[FILE2[:..]]
     Phrase translation model file(s) in TPPT format (Tightly Packed Phrase
     Table), indexed on the source language, containing an even number of
     models, considered to be backward models followed by the same number of
     forward models.
     Like a multi-prob phrase table, a TPPT contining 2*N probs requires N
     backward weights and, if they are supplied, N forward weights.

 Phrase table notes:
     At least one translation model must be specified (through the
     -ttable-tppt or -ttable-file* options).
     The number of translation models must match the number of translation
     model weights.

 -lmodel-file FILE1[:FILE2[:..]]
     The language model file(s).  At least one file must be specified
     (either in the CONFIG file or on the command line).  The number of
     language model files specified must match the number of language
     model weights.  Note that the file list is delimited by ':'.
     The order of each language model will be determined by inspection,
     but you can restrict the order of a model by adding #N to its name.
     E.g., if 4g.lm is a 4-gram model, specifying 4g.lm#3 will treat it as
     a 3-gram model.

     Warning: all LM formats specify the use of base-10 log probs, but canoe
     interprets them as natural log.  This known bug has minimal impact;
     correcting it requires multiplying the desired -weight-l values by
     log(10), which cow/rat/rescore_train do implicitly.  We chose not to
     fix it to avoid having to adjust all previously tuned sets of weights.
     Note that throughout Portage, logs are natural by default, not base 10.

 -lmodel-order LMORDER
     If non-zero, globally limits the order of all language models. [0]

 -weight-t W1[:W2[:..]]
 -tm W1[:W2[:..]]
     The translation model weight(s).  If this option is used, the number of
     weights must match the number of translation models specified.  [1.0]

     If a mix of text, multi-prob and TPPT translation models are
     specified, the weights of the single-prob text translation models come
     first, then the weights of the multi-prob models, finally the weights of
     the TPPT models.

 -weight-f W1[:W2[:..]]
 -ftm W1[:W2[:..]]
     The weight(s) for the forward probabilities in the translation models.
     If this option is used, it has the same requirements as -weight-t.
     [none]

 -weight-a W1[:W2[:..]]
 -atm W1[:W2[:..]]
     The weight(s) for the adirectional scores in the translation models.
     If this option is used, it must supply the same number of weights as
     there are adirectional features in all the translation models.
     [none]

 -weight-l W1[:W2[:..]]
 -lm W1[:W2[:..]]
     The language model weight(s).  If this option is used, the number of
     weights must match the number of language models specified.  [1.0]
     Note: see warning under -lmodel-file for details on the numerical
     interpretation of this parameter.

 -weight-lev W
 -lev W
     Weight for Levenshtein distance when the reference is used in canoe.

 -weight-ngrams W1[:W2[:..]]
 -ng W1[:W2[:..]]
     Weight for n-gram precision when the reference is used in canoe.

 -weight-d W1[:W2[:..]]
 -d W1[:W2[:..]]
     The distortion model weight(s).  [1.0]

 -weight-w W
 -w W
     The sentence length weight.  [0.0]

 -weight-s W
 -sm W
     The segmentation model weight.  [none]

 -weight-ibm1-fwd W
 -ibm1f W
     The forward IBM1 feature weight (see -ibm1-fwd-file). [none]

 -random-weights
 -r
     Ignore given weights, set weights randomly for each sentence instead
     [don't]

 -seed N
     Set (positive integer) random seed (see -random-weights).  [0]

 -stack S
 -s S
     The hypothesis stack size.  [100]

 -beam-threshold T
 -b T
     The hypothesis stack relative threshold [0.0001]

 -cov-limit COV_L
     The coverage pruning limit (max number of states to keep with identical
     coverage) (use 0 for no limit) (example value: 10) [0, i.e., no limit]

 -cov-threshold COV_T
     The coverage pruning threshold (max ratio of probability between top
     and lowest state with identical coverage) (use 0.0 for no threshold)
     (example value: 0.1) [0.0, i.e., no threshold]

 -ttable-limit L
     The number of target phrases to keep in translation table pruning; 0
     means no limit.  If used, forward probs should be provided (via the
     -ttable-* options); ttable pruning is arbitrary otherwise. [0]

 -ttable-threshold T
     The translation table pruning threshold; 0.0 means no threshold.  If
     non-0, target phrases with a score (defined via -ttable-prune-type
     below) less than log(T) are discarded.  Also has arbitrary effects if
     forward probs are not available.

 -ttable-prune-type PRUNE_TYPE
     Semantics of the L and T parameters above.  Pruning is done using:
     'backward-weights': forward probs (if available) with backward weights
     'forward-weights': forward probs with forward weights
     'combined': the combination of forward and backward probs with their
     respective weights.
     [forward-weights if -weight-f is supplied, backward-weights otherwise]

 -ttable-log-zero LZ
     The log probability assigned to phrase pairs that either don't occur
     within some phrasetable, or that have 0 probability. [-18]

 -levenshtein-limit levL
     The maximum levenshtien distance between current translation, or -1 for
     no limit. The system will return an error if no translation can be found
     that respects levL. [-1]

 -distortion-limit L
     The maximum distortion distance between two source words, or -1 for
     no limit.  Use 0 to do monotonic decoding.  [-1]
     See -dist-limit-ext for further details on the semantics of L.

 -dist-limit-ext
     Use the 'Extended' distortion limit definition.

     Suppose the previous source phrase spans [a,b), the current source phrase
     spans [c,d), and the first non-covered source position is NC1.

     By default, the distortion limit is respected iff
        |c-b| <= L AND d <= NC1 + L
     i.e., if the new jump respects the distortion limit and the new phrase
     doesn't *end* past NC1 + L.

     The extended distortion limit changes the second condition to require that
     the new phrase doesn't *begin* past NC1 + L:  With -dist-limit-ext, the
     distortion limit is respected iff
        |c-b| <= L AND c <= NC1 + L

     In both cases, dead-end hypotheses (partial hypotheses that can't be
     completed without eventually violating the distortion limit) are pruned
     as soon as they can be detected.

 -dist-phrase-swap
     Allow swapping two contiguous source phrases of any length. [don't]
     Applied as an OR with the distortion limit:  meaningless if L = -1,
     yields quasi-monotonic decoding if L = 0, and a targetted relaxing of
     the distortion limit rule if L > 0.  Orthogonal with -dist-limit-ext.

 -distortion-model model[#args][:model2[#args][:..]]
     The distortion model(s) and their arguments. Zero or more of:
     WordDisplacement, PhraseDisplacement,
     fwd-lex[#dir], back-lex[#dir], ZeroInfo.
     To get no distortion model, use 'none'.  [WordDisplacement]

     The lexicalized distortion models (LDM), fwd-lex and back-lex, take an
     optional direction argument, which can be m (monotone), s (swap) or
     d(discontinuous).
     To combine the distortion penalty with a 2-feature LDM, use:
        WordDisplacement:back-lex:fwd-lex
     To combine the distortion penalty with a 6-feature LDM, use:
        WordDisplacement:back-lex#m:back-lex#s:back-lex#d:fwd-lex#m:fwd-lex#s:fwd-lex#d

 -lex-dist-model-file FILE1[:FILE2[:..]]
     The lexicalized distortion model file(s) in multi-prob text format.
     The parameter FILE is formatted like a phrase table, but with different
     semantics:
     src phrase ||| tgt phrase ||| pm ps pd nm ns nd
     Where p=prev, n=next and m=monotonic, s=swap and d=discontinuous.

 -segmentation-model model[#args]
     The segmentation model: one of none, count, bernoulli. [none]
     Some models require an argument, introduced by '#':
     - bernoulli requires a numerical argument (Q parameter)

 -ibm1-fwd-file file
     Use 'forward' IBM1 feature - file should be an IBM1 model trained for
     target language given source language. [none]

 -bypass-marked
     When marked translations are found in the source text, translation
     options from the translation table are not excluded.

 -weight-marked W
     A weight to multiply marked translation probabilites by.  [1.0]

 -oov method
     How to handle out-of-vocabulary source words. Method is one of:
        pass - pass them through to target translation;
        write-src-marked - DON'T TRANSLATE, just write the source text
           with OOVs marked like this: <oov>the-oov</oov>
        write-src-deleted - DON'T TRANSLATE, just write the source text
           with OOVs stripped out.
     [pass]

 -tolerate-markup-errors
     When invalid markup is found, attempt to interpret it as literal text.
     This does a minimal effort only, so parts of the invalid input will
     typically be lost.  [abort on invalid markup]

 -check-input-only
     Just check the input for markup errors, don't read any models or decode
     anything.  With this option, a non-zero exit status indicates fatal
     format errors were found.

 -backwards
     Forms the translation from end to start instead of start to end.
     The language model should be trained on a backwards corpus.

 -load-first
     Loads the models before reading the source sentences, so that
     translations are produced on the fly.  If this is not used, source
     sentences are read first and only applicable entries are stored from
     the phrase table and language model files.

 -palign
 -trace
 -t
     Produce alignment and OOV output. If -lattice is given, this info
     will also be stored in the lattice. If -nbest is given, alignment
     info (but not OOV) will be written along with the nbest list.

 -ffvals
     Produce feature function output. If -lattice or -nbest is given,
     this info will also be written to the lattice or nbest list.

 -lattice LPREFIX[.gz]
     Produces word graph output into files LPREFIX.SENTNUM[.gz], where
     SENTNUM is a 4+ digit representation of the sentence number, starting
     at 0000 (or the value of -first-sentnum).  State coverage vectors are
     output into LPREFIX.SENTNUM.state[.gz].  If -trace or -ffvals is
     specified then this form of output is used in the wordgraph as well.
     Even if -backwards is specified, the word graph gives forwards
     sentences.  If .gz is specified, the output will be gzipped.

 -nbest NPREFIX[.gz][:N]
     Produces nbest output into files NPREFIX.SENTNUM.Nbest[.gz]. If N is
     not specified, 100 is used. If -ffvals is also specified, feature
     function values are written to NPREFIX.SENTNUM.Nbest.ffvals[.gz].  With
     -trace, alignment info is written to NPREFIX.SENTNUM.Nbest.pal[.gz].
     If .gz is specified, the outputs will be gzipped.

 -first-sentnum INDEX
     Indicates the first SENTNUM to use in creating the file names for
     the -lattice and -nbest output.  [0000]

 -input FILE
     The source sentences file.  [-]

 -ref   FILE
     The reference target sentences file.

 -append
     The decoder will output its results in a single file instead of one
     file per source sentences.  This will automatically be applied to
     nbest, ffvals, pal, lattice and lattice_state

 -lb
     Indicates that canoe is running in load-balancing mode thus the source
     sentences will be prepended with a source sentence id.

 -cube-pruning
     Run the cube pruning decoder, a la Huang+Chiang (ACL 2007), instead of
     the regular stack decoder.  (Not compatible yet with coverage pruning).
     Note that with regular decoding, each stack gets up to S elements, not
     counting recombined states, whereas with cube pruning recombined states
     are counted.  Recomemded S values are therefore around 10000-30000 with
     cube pruning, rather than 100-300.

 -future-score-lm-heuristic FUT-LM-HEURISTIC
     Specify the LM heuristic to use for the future score.  One of:
        none - h = 1.0;
        unigram - h = unigram probabilities;
        simple - h = 1.0 if context is partial, h = prob otherwise;
        incremental - h = unigram for first word, bigram for next, etc.;
     [incremental]

 -future-score-use-ftm
     Also use forward translation model probabilities to compute the
     future costs during decoding.  [do] (use -no-future-score-use-ftm to
     turn this behaviour off).

 -cube-lm-heuristic CUBE-LM-HEURISTIC
     Specify the LM heuristic to use for cube pruning.  Same choices as
     above.  Affects the order in which the cube pruning decoder considers
     candidate phrases, whereas FUT-LM-HEURISTIC is used to calculate the
     global future score [incremental]

 -rule-classes class1[:class2[:..]]
 -ruc class1[:class2[:..]]
     Lists allowed rule classes in source text.

 -rule-weights W1[:W2[:..]]
 -ruw W1[:W2[:..]]
     Weights associated with each rule classes.

 -rule-log-zero E1[:E2[:..]]
 -rulz E1[:E2[:..]]
     Log value of epsilon for each rule classes.  These values are used
     when a rule knows about the source phrase but the target phrase is not
     the one specified in the rule.

 -final-cleanup
     Indicates to clear the bmg when canoe is done [false].
     For speed, we normally don't delete the bmg at the end of canoe, but
     for some debugging deleting the bmg might be appropriate.

 -bind PID
     Binds this instance of canoe to the existence of PID running: when PID
     disappears, canoe will exit automatically with exit status 45.

 -verbose V
 -v V
     The verbosity level (1, 2, 3, or 4).  All verbose output is written to
     standard error.

 -options
     Produce a shorter help message only listing the option names

 NOTES:

   - All options of the form weight-X where X is a decoder feature can be
     replaced by random-X with a value of U(min, max) or N(mean, sigma).  This
     allows canoe to use Uniform or Normal distribution to determine the
     weights of the decoder feature for each source sentence.  Intended for use
     with -random-weights.  If no distribution is specified, U(-1.0, 1.0) is
     used as a default value.  Example:
        [random-t] U(0,3):N(2,0.1):...

   - All boolean options (i.e., all the switches that don't take an argument)
     can be reversed: -no-OPT reverses -OPT.  Options set in the CONFIG file
     can be overridden on the command line with this syntax.

   - If you have a hard time reading this help message in your terminal, try
     bash: canoe -h 2>&1 | less
     tcsh: canoe -h |& less

