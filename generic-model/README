                         Portage Generic Model 2.0

Traitement multilingue de textes / Multilingual Text Processing
Centre de recherche en technologies numÃ©riques / Digital Technologies Research Centre
Conseil national de recherches Canada / National Research Council Canada
Copyright 2016, 2018, Sa Majeste la Reine du Chef du Canada
Copyright 2016, 2018, Her Majesty in Right of Canada

Distributed under specific licensing terms.  Please refer to your signed
license agreement for details.


                         Portage Generic Model 2.0

If you have licensed the Portage Generic Models, you should install these
models in the generic-model directory (see section Installing... below).

The Portage Generic Model DVD contains trained English and French language
models (LM), English->French and French->English translation models (TM), and
English->French and French->English Neural Network Joint Models (NNJM), which
may be used in conjunction with your own in-domain language and translation
models in a tuned PortageII system. The generic models were trained on a large
general language corpus and then pruned, keeping the most relevant data while
shrinking the models to a usable size.

The training corpus used to create these models consists of 43.7 million
sentence pairs collected from all Government of Canada web sites (our "gc.ca"
corpus) in 2009. It covers a multitude of domains since the federal government
is concerned with a lot of different subjects.

Using the generic models in conjunction with your in-domain models can
significantly improve translation quality of new text, especially for small
in-domain corpora: the generic models will help the system handle the general
constructs of the language, while the in-domain material will provide the
domain-specific vocabulary and constructions.

We recommend you systematically use the generic LM for systems translating into
French or English, since that model will help construct better sentences from
your own in-domain text, but will never introduce terminology that does not
exist in your in-domain training corpus.

We also recommend the use of the generic TM for systems between French and
English, in either direction. A system trained this way will have a very large
vocabulary and handle unseen expressions and words much better. It can produce
good quality MT output even with a fairly small in-domain training corpus.
However, if the occasional insertion of terminology that is not from the
in-domain corpus is not acceptable, leave out the generic TM and use only the
generic LM.

We thank Library and Archives Canada for providing the data on which the models
for Portage Generic Model 1.0 and 2.0 were trained.


             Changes between Portage Generic Model 1.0 and 2.0

NNJM: Neural Network Joint Model

   PortageII 3.0 introduces the Neural Network Joint Model (NNJM), trained
   using Deep Learning technology. While the training software for NNJMs is not
   released yet, Portage Generic Model 2.0 includes a pre-trained NNJM for
   English->French and one for French->English.

Size of the training data

   We improved our sentence-pair extraction tools, so that 42.5 million
   sentence pairs from the the gc.ca corpus were used to create version 2.0 of
   the generic models, while 31 million were available for version 1.0.

   This larger training corpus was used to update the LM and to create the new
   NNJM.

   The updated TM is not available yet, so Portage Generic Model 2.0 includes a
   copy of the generic TM from 1.0. An updated version of the TM should be
   available in a future release.

Usage recommendation

   We now recommend always using the generic LM for systems into English or
   French.


                   Contents of the generic-model directory

Note: Portage Generic Model 2.0 takes 7GB, so you might have it on one
double-layer DVD or on two regular DVDs: in that case, disk1 has all the files
required for using Portage Generic Model 2.0 as background models, while disk2
provides the same models in different formats, for reference and alternative
uses.

   generic-2.0/
      lm/      English and French Language Models
         generic-2.0_en.tplm/ (disk 1)
         generic-2.0_fr.tplm/ (disk 1)
               English and French LMs in TPLM format, used both for training
               systems and for translating into English and into French,
               respectively.

         generic-2.0_en.binlm.gz (disk 2)
         generic-2.0_fr.binlm.gz (disk 2)
               English and French LMs in binlm format (now for reference only;
               required for training with PortageII 2.2 or older)

      nnjm/    English->French and French->English Neural Network Joint Models
         nnjm.generic-2.0.en2fr.mm/ (disk 1)
         nnjm.generic-2.0.fr2en.mm/ (disk 1)
               Pre-trained NNJMs for use with English->French and
               French->English systems, respectively.

   generic-1.0/
      tm/      English->French and French->English Translation Models
         cpt.generic-1.0.en2fr.gz (disk 1)
         cpt.generic-1.0.fr2en.gz (disk 1)
               TMs for training English->French and French->English translation
               systems, respectively.

         cpt.generic-1.0.en2fr.tppt/ (disk 2)
         cpt.generic-1.0.fr2en.tppt/ (disk 2)
               The same models, in tightly-packed format (TPPT), used for our
               pre-tuned generic model.

   README   this file


                    Installing Portage Generic Model 2.0

If you have licensed the Portage Generic Model DVD, you can install it in the
generic-model directory.

Copy the full contents of first DVD (disk 1) or the double-layer DVD to the
generic-model directory within PortageII: $PORTAGE/generic-model.
   cp -prd /DVD1location/PortageGenericModel-2.0/* $PORTAGE/generic-model/
or
   rsync -av /DVD1location/PortageGenericModel-2.0/* $PORTAGE/generic-model/
Either command above should create $PORTAGE/generic-model/generic-2.0 and
$PORTAGE/generic-model/generic-1.0, the former containing the new generic LM and
and NNJM, the latter the copy of the old generic TM.

You also need to make a copy of disk 1 or the double-layer DVD on every
PortageLive server you use, under /opt/PortageII/models/pretrained:
   cp -prd /DVD1location/PortageGenericModel-2.0/* /opt/PortageII/models/pretrained/
or
   rsync -av /DVD1location/PortageGenericModel-2.0/* hostname:/opt/PortageII/models/pretrained/

You can optionally copy the second DVD (disk 2) into the same directory. Either
of the following commands will merge the second DVD contents into the structure
where you copied the first:
   cp -prd /DVD2location/PortageGenericModel-2.0/* $PORTAGE/generic-model/
or
   rsync -av /DVD2location/PortageGenericModel-2.0/* $PORTAGE/generic-model/


                     The PORTAGE_GENERIC_MODEL variable

The framework uses variable PORTAGE_GENERIC_MODEL to find the generic models.
By default, it sets PORTAGE_GENERIC_MODEL=$PORTAGE/generic-model. If you
installed the models elsewhere, modify SETUP.bash to define
PORTAGE_GENERIC_MODEL accordingly.


                               Getting Started

Once installed, the models of the Portage Generic Model can be used in
conjunction with your in-domain models in the PortageII framework.

You can use the language models in the lm directory in standard (log-linear)
combination with your own in-domain LMs or in linear combination using a MixLM.
To do so, prior to training your PortageII system edit your framework
Makefile.params file as follows.

To train a PortageII system using the generic LM in standard combination with
your in-domain LMs, add the file path of the TPLM format target language
generic model LM to the LM_PRETRAINED_TGT_LMS variable definition. For example,
for an English->French system, this would be:

LM_PRETRAINED_TGT_LMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.0/lm/generic-2.0_fr.tplm

To train a PortageII system using the generic LM in linear combination with your
in-domain LMs in a MixLM, add the file path of the TPLM format target language
generic model LM to the MIXLM_PRETRAINED_TGT_LMS variable definition. For
example, for an English->French system, this would be:

MIXLM_PRETRAINED_TGT_LMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.0/lm/generic-2.0_fr.tplm

You can use the translation models (CPTs) in the tm directory in standard (log-
linear) combination with your own in-domain CPTs or in a linear combination
using a MixTM CPT. To do so, prior to training your PortageII system, edit
your framework Makefile.params file as follows.

To train a PortageII system using the generic TM in standard combination with
your in-domain TMs, add the file path of the en2fr generic model CPT (for an
English->French system) or fr2en generic model CPT (for a French->English
system) to the TM_PRETRAINED_TMS variable definition. For example, for an
English->French system, this would be:

TM_PRETRAINED_TMS ?= ${PORTAGE_GENERIC_MODEL}/generic-1.0/tm/cpt.generic-1.0.en2fr.gz

To train a PortageII system using the generic TM in linear combination with your
in-domain TMs in a MixTM, add the file path of the en2fr generic model CPT (for
an English->French system) or fr2en generic model CPT (for a French->English
system) to the MIXTM_PRETRAINED_TMS variable definition. For example, for an
English->French system, this would be:

MIXTM_PRETRAINED_TMS ?= ${PORTAGE_GENERIC_MODEL}/generic-1.0/tm/cpt.generic-1.0.en2fr.gz

The framework will automatically create symbolic links to the necessary generic
model files within the framework structure, including the LM in TPLM format for
PortageLive, and the source language LM if needed for training a MixLM.

In all cases, do not include the generic model name in the TRAIN_LM, MIXLM,
TRAIN_TM, or MIXTM definitions in the Makefile.params file.

Note: All four PRETRAINED variables accept multiple file paths, so you may add
your own pre-trained models as well.

Our recommended practice is to use both the generic model LM and generic model
TM, both in linear combination with your in-domain models. Our experience is
that the linear mixtures (MixLM and MixTM) yield a system with better
translation quality than the standard log-linear combination. If you have a
situation where translation time is an issue, say for single-sentence-at-a-time
translation within a short time window, we recommend that you experiment with
trying out and benchmarking various combinations of standard and mixture models
to see what works best for you. In some situations the standard models may
result in quicker translation.


                  Simplest use: one corpus plus generic models

If you want to use the generic models in a MixLM and MixTM combination with a
single, in-domain corpus, here is what you would do in Makefile.params.  We'll
assume your LM training corpus is called lm-train, and your TM training corpus
is called tm-train, as is the default in the framework.

1. MixLM-related variables

   a. Comment out the line defining TRAIN_LM, since your in-domain corpus will
      be in the MixLM, not as a regular LM:

      #TRAIN_LM ?= lm-train

   b. Uncomment and change the line defining MIXLM to specify only your corpus:

      MIXLM ?= lm-train

   c. Uncomment the line defining MIXLM_PRETRAINED_TGT_LMS:

      MIXLM_PRETRAINED_TGT_LMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.0/lm/generic-2.0_fr.tplm

   d. TRAIN_TC now needs to be explicitly defined, because its default is to
      copy TRAIN_LM, which is no longer defined.  Uncomment and edit the line
      defining it:

      TRAIN_TC ?= lm-train

2. MixTM-related variables

   a. As for LMs, turn off the regular TM training by commenting out the
      definition of TRAIN_TM:

      #TRAIN_TM ?= tm-train

   b. Add your TM training corpus to MIXTM instead:

      MIXTM ?= tm-train

   c. Uncomment the line defining MIXTM_PRETRAINED_TMS:

      MIXTM_PRETRAINED_TMS ?= ${PORTAGE_GENERIC_MODEL}/generic-1.0/tm/cpt.generic-1.0.en2fr.gz

   d. Leave MIXTM_TRAIN_MIX undefined (don't uncomment its line) since its
      default value is appropriate:

      #MIXTM_TRAIN_MIX ?= subtm1


