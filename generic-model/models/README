                         Portage Generic Model 2.1

Traitement multilingue de textes / Multilingual Text Processing
Centre de recherche en technologies numériques / Digital Technologies Research Centre
Conseil national de recherches Canada / National Research Council Canada
Copyright 2016, 2018, Sa Majesté la Reine du Chef du Canada
Copyright 2016, 2018, Her Majesty in Right of Canada

Distributed under specific licensing terms.  Please refer to your signed
license agreement for details.


                         Portage Generic Model 2.1

If you have licensed the Portage Generic Models, you should install these
models in the generic-model directory (see section Installing... below).

The Portage Generic Model DVD contains trained English and French language
models (LM), English->French and French->English translation models (TM), and
English->French and French->English Neural Network Joint Models (NNJM), which
may be used in conjunction with your own in-domain language and translation
models in a tuned PortageII system. The generic models were trained on a large
general language corpus and then pruned, keeping the most relevant data while
shrinking the models to a usable size.

The training corpus used to create these models consists of 43.7 million
sentence pairs collected from all Government of Canada web sites (our "gc.ca"
corpus) in 2009. It covers a multitude of domains since the federal government
is concerned with a lot of different subjects.

Using the generic models in conjunction with your in-domain models can
significantly improve translation quality of new text, especially for small
in-domain corpora: the generic models will help the system handle the general
constructs of the language, while the in-domain material will provide the
domain-specific vocabulary and constructions.

We recommend you systematically use the generic LM for systems translating into
French or English, since that model will help construct better sentences from
your own in-domain text, but will never introduce terminology that does not
exist in your in-domain training corpus.

We also recommend the use of the generic TM for systems between French and
English, in either direction. A system trained this way will have a very large
vocabulary and handle unseen expressions and words much better. It can produce
good quality MT output even with a fairly small in-domain training corpus.
However, if the occasional insertion of terminology that is not from the
in-domain corpus is not acceptable, leave out the generic TM and use only the
generic LM.

The generic NNJM will help construct better sentences by considering a window
of source words and the more recent target words. Whether it helps your system
is a matter of experimentation, try it and see. Like the generic LM, it will
not introduce external terminology, so its use is safe even if you want to
preserve pure terminology.

We thank Library and Archives Canada for providing the data on which the models
for Portage Generic Model 1.0 to 2.1 were trained.


                History of changes for Portage Generic Model

Portage Generic Model 2.1 (July 2018)
   Updated TM trained on the whole 43.7 million sentence pairs and well
   optimized for quality and size.

   Updated NNJM better optimized for quality and for fine-tuning, which is now
   possible at client sites with PortageII 4.0.

   The LM remains the same as with 2.0, but we renamed it to 2.1 to keep the
   naming convention in this model consistent throughout.

   Fully tuned, ready-to-deploy generic systems accompany the generic models.

Portage Generic Model 2.0 (July 2016)

   NNJM: Neural Network Joint Model

      PortageII 3.0 introduces the Neural Network Joint Model (NNJM), trained
      using Deep Learning technology. While the training software for NNJMs is
      not released yet, Portage Generic Model 2.0 includes a pre-trained NNJM
      for English->French and one for French->English.

   Size of the training data

      We improved our sentence-pair extraction tools, so that 42.5 million
      sentence pairs from the the gc.ca corpus were used to create version 2.0
      of the generic models, while 31 million were available for version 1.0.

      This larger training corpus was used to update the LM and to create the
      new NNJM.

      The updated TM is not available yet, so Portage Generic Model 2.0
      includes a copy of the generic TM from 1.0. An updated version of the TM
      should be available in a future release.

   Usage recommendation

      We now recommend always using the generic LM for systems into English or
      French.

Portage Generic Model 1.1 (April 2016)

   Optimization of the generic 1.0 systems for translation speed (made them
   about ten times faster).

Portage Generic Model 1.0 (September 2012)

   Initial release of the generic LM and TM for French<->English.
   Initial release of the French<->English fully trained, ready-to-deploy
   systems.


                   Contents of the generic-model directory

Note: Portage Generic Model 2.1 takes 7GB, distributed as a tar ball on a
double-layer DVD, including all the files required for using Portage Generic
Model 2.1 as background models, and addition formats for reference and
alternative uses.

   generic-2.1/
      lm/      English and French Language Models
         generic-2.1_en.tplm/
         generic-2.1_fr.tplm/
               English and French LMs in TPLM format, used both for training
               systems and for translating into English and into French,
               respectively. (Same as generic-2.0)

      nnjm/    English->French and French->English Neural Network Joint Models
         nnjm.generic-2.1.en2fr/
         nnjm.generic-2.1.fr2en/
               Pre-trained NNJMs for use with English->French and
               French->English systems, respectively. (Updated)

      tm/      English->French and French->English Translation Models
         cpt.generic-2.1.en2fr.gz
         cpt.generic-2.1.fr2en.gz
               TMs for training English->French and French->English translation
               systems, respectively. (Updated)

         cpt.generic-2.1.en2fr.tppt/
         cpt.generic-2.1.fr2en.tppt/
               The same models, in tightly-packed format (TPPT), used for our
               pre-tuned generic model.

   README   this file
   LICENCE


                    Installing Portage Generic Model 2.1

If you have licensed the Portage Generic Model DVD, install it in the
generic-model directory.

Expand the tar ball on the DVD to a location of your choice:
   tar -xvzf /path/to/PortageGenericModel-2.1.tar.gz
then symlink, copy or move its subdirectory generic-2.1 to the generic-model
directory within PortageII: $PORTAGE/generic-model:
   cd $PORTAGE/generic-model/
   ln -s /path/to/PortageGenericModel-2.1/generic-2.1
or
   cp -prd PortageGenericModel-2.1/generic-2.1 $PORTAGE/generic-model/
or
   rsync -av PortageGenericModel-2.1/* $PORTAGE/generic-model/
or
   mv PortageGenericModel-2.1/generic-2.1 $PORTAGE/generic-model/

All commands above should create $PORTAGE/generic-model/generic-2.1 containing
or pointing to the new generic LM, NNJM, and TM.

You also need to make a copy of the generic-2.1 directory on every
PortageLive server you use, under /opt/PortageII/models/pretrained:
   cd /opt/PortageII/models/pretrained/
   ln -s /path/to/PortageGenericModel-2.1/generic-2.1
or
   cp -prd $PORTAGE/generic-model/generic-2.1 /opt/PortageII/models/pretrained/
or
   rsync -av $PORTAGE/generic-model/generic-2.1 hostname:/opt/PortageII/models/pretrained/


                     The PORTAGE_GENERIC_MODEL variable

The framework uses variable PORTAGE_GENERIC_MODEL to find the generic models.
By default, it sets PORTAGE_GENERIC_MODEL=$PORTAGE/generic-model. If you
installed the models elsewhere, modify SETUP.bash to define
PORTAGE_GENERIC_MODEL accordingly.


                               Getting Started

Once installed, the models of the Portage Generic Model can be used in
conjunction with your in-domain models in the PortageII framework.

   LM

You can use the language models in the lm directory in standard (log-linear)
combination with your own in-domain LMs or in linear combination using a MixLM.
To do so, prior to training your PortageII system edit your framework
Makefile.params file as follows.

      LM - MixLM (Recommended)

To train a PortageII system using the generic LM in linear combination with your
in-domain LMs in a MixLM, add the file path of the TPLM format target language
generic model LM to the MIXLM_PRETRAINED_TGT_LMS variable definition. For
example, for an English->French system, this would be:

MIXLM_PRETRAINED_TGT_LMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/lm/generic-2.1_fr.tplm

Since PortageII 3.0, you can accomplish the same thing automatically by setting
you LM training corpus in PRIMARY_LM instead of using MIXLM (or TRAIN_LM): if
it exists for your target language, the generic LM will be automatically
combined with your LM in a MixLM.

      LM - log-linear combination

To train a PortageII system using the generic LM in standard combination with
your in-domain LMs, add the file path of the TPLM format target language
generic model LM to the LM_PRETRAINED_TGT_LMS variable definition. For example,
for an English->French system, this would be:

LM_PRETRAINED_TGT_LMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/lm/generic-2.1_fr.tplm

   TM

You can use the translation models (CPTs) in the tm directory in standard (log-
linear) combination with your own in-domain CPTs or in a linear combination
using a MixTM CPT. To do so, prior to training your PortageII system, edit
your framework Makefile.params file as follows.

      TM - MixTM (recommended)

To train a PortageII system using the generic TM in linear combination with your
in-domain TMs in a MixTM, add the file path of the en2fr generic model CPT (for
an English->French system) or fr2en generic model CPT (for a French->English
system) to the MIXTM_PRETRAINED_TMS variable definition. For example, for an
English->French system, this would be:

MIXTM_PRETRAINED_TMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/tm/cpt.generic-2.1.en2fr.gz

      TM - log-linear combination

To train a PortageII system using the generic TM in standard combination with
your in-domain TMs, add the file path of the en2fr generic model CPT (for an
English->French system) or fr2en generic model CPT (for a French->English
system) to the TM_PRETRAINED_TMS variable definition. For example, for an
English->French system, this would be:

TM_PRETRAINED_TMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/tm/cpt.generic-2.1.en2fr.gz

   NNJM

You can use the generic NNJM on its own or fine-tune it using your own
in-domain data, which is our recommendation. How to do so is documented in
details in Makefile.params, in the NNJM training section.

      NNJM - using the pretrained NNJM on its own

To use the pre-trained NNJM on its own, uncomment this line in Makefile.params:

NNJM_PRETRAINED_NNJM ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/nnjm/nnjm.generic-2.1.${SRC_LANG}2${TGT_LANG}.mm/model

      NNJM - fine-tuning with your own data

Typical example, assuming tm-train is your main in-domain training corpus:

NNJM_FINE_TUNING_TRAIN_CORPUS ?= tm-train
NNJM_FINE_TUNING_DEV_CORPUS ?= ${TUNE_DECODE}
NNJM_PRETRAINED_NNJM ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/nnjm/nnjm.generic-2.1.${SRC_LANG}2${TGT_LANG}.mm/model



The framework will automatically create symbolic links to the necessary generic
model files within the framework structure, including the LM in TPLM format for
PortageLive, and the source language LM if needed for training a MixLM.

In all cases, do not include the generic model name in the PRIMARY_LM,
TRAIN_LM, MIXLM, TRAIN_TM, or MIXTM definitions in the Makefile.params file.

Note: All four PRETRAINED variables for LMs and TMs accept multiple file paths,
so you may add your own pre-trained models as well.

Our recommended practice is to use both the generic model LM and generic model
TM, both in linear combination with your in-domain models, and fine tune the
NNJM on your data. Our experience is that the linear mixtures (MixLM and MixTM)
yield a system with better translation quality than the standard log-linear
combination. If you have a situation where translation time is an issue, say
for single-sentence-at-a-time translation within a short time window, we
recommend that you experiment with trying out and benchmarking various
combinations of standard and mixture models to see what works best for you. In
some situations the standard models may result in quicker translation.


                  Simplest use: one corpus plus generic models

If you want to use the generic models in a MixLM and MixTM combination with a
single, in-domain corpus, here is what you would do in Makefile.params.  We'll
assume your LM training corpus is called lm-train, and your TM training corpus
is called tm-train, as is the default in the framework.

1. MixLM-related variables

   a. Comment out the line defining TRAIN_LM, since your in-domain corpus will
      be in the MixLM, not as a regular LM:

      #TRAIN_LM ?= lm-train

   Variant 1 (recommended):

   b. Define PRIMARY_LM to point to your LM training corpus:

      PRIMARY_LM ?= lm-train

   Variant 2 (still functional):

   b. Uncomment and change the line defining MIXLM to specify only your corpus:

      MIXLM ?= lm-train

   c. Uncomment the line defining MIXLM_PRETRAINED_TGT_LMS:

      MIXLM_PRETRAINED_TGT_LMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/lm/generic-2.1_${TGT_LANG}.tplm

   d. TRAIN_TC now needs to be explicitly defined, because its default is to
      copy TRAIN_LM, which is no longer defined.  Uncomment and edit the line
      defining it:

      TRAIN_TC ?= lm-train

2. MixTM-related variables

   a. As for LMs, turn off the regular TM training by commenting out the
      definition of TRAIN_TM:

      #TRAIN_TM ?= tm-train

   b. Add your TM training corpus to MIXTM instead:

      MIXTM ?= tm-train

   c. Uncomment the line defining MIXTM_PRETRAINED_TMS:

      MIXTM_PRETRAINED_TMS ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/tm/cpt.generic-2.1.${SRC_LANG}2${TGT_LANG}.gz

   d. Leave MIXTM_TRAIN_MIX undefined (don't uncomment its line) since its
      default value is appropriate:

      #MIXTM_TRAIN_MIX ?= subtm1

   e. TRAIN_SPARSE now needs to be explicitly defined, because its default is
      to copy TRAIN_TM, which is no longer defined.  Uncomment and edit the line
      defining it:

      TRAIN_SPARSE ?= tm-train

3. Variables for the Fine-tuned NNJM

   a. Define NNJM_FINE_TUNING_TRAIN_CORPUS to your main in-domain training
      corpus, tm-train, and NNJM_FINE_TUNING_DEV_CORPUS to your decoding dev
      set, ${TUNE_DECODE}:

      NNJM_FINE_TUNING_TRAIN_CORPUS ?= tm-train
      NNJM_FINE_TUNING_DEV_CORPUS ?= ${TUNE_DECODE}

   b. Turn on the generic NNJM by uncommenting the line defining it:

      NNJM_PRETRAINED_NNJM ?= ${PORTAGE_GENERIC_MODEL}/generic-2.1/nnjm/nnjm.generic-2.1.${SRC_LANG}2${TGT_LANG}.mm/model


