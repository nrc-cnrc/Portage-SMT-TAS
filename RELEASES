                            PORTAGEshared_v1.3c
           (unofficial release of the current development trunk)

Technologies langagieres interactives / Interactive Language Technologies
Institut de technologie de l'information / Institute for Information Technology
Conseil national de recherches Canada / National Research Council Canada
Copyright 2004-2008, Sa Majeste la Reine du Chef du Canada
Copyright 2004-2008, Her Majesty in Right of Canada

Distributed under specific licensing terms.  Please refer to your signed
license agreement for details.


                              Release History
                   (with a summary of important changes)

Rough notes as I go
 - gale and uli word alignment format writers
 - IBMDiagAligner, HybridPostAligner and ExternalAligner word aligners
 - phrase smoothers: JointFreqs, alpha-smoothing option to RFSmoother and
   IndicatorSmoother
 - Moses-style lexicalized distortion
 - decoder discard-recomb when only asking for 1-best
 - decoder lev / ngram / adir / LDM

Current development trunk
 This sections shows changes since the latest official release.

 Major changes:

 - New script tmtext-apply-weights.pl pre-applies log-linear weights learned
   using cow.sh to create more compact models for use by a translation server.

 - The phrase extraction process (gen_phrase_tables) has been modified to
   require that a phrase pair have at least one actually linked word pair.
   (Previously, unaligned words were allowed to be considered a phrase pair.)

 Minor changes:

 - New "sri" word alignment reader (for word_align_tool, eval_word_alignment).
 - The forward TM scores are now included in future score calculation.
 - Canoe now supports the -bind PID option, exiting automatically when the
   master process disappears.
 - The tokenizer now takes linear time regardless of paragraph length (used to
   be quadratic on the length of each paragraph).
 - Minor improvements to run-parallel.sh.
 - Save dependency information per source file during compilation instead of
   re-processing each file to generate Makefile.depends.
 - Some code clean-up using Klocwork Insight, fixing potential future problems.
 - Some code documentation clean-up.

v1.3 21.01.2009
 This update to PORTAGEshared is primarily intended to incorporate the new HMM
 word alignment module, and related functionality.  We have also taken the
 opportunity to migrate many improvements from Portage, add a new experimental
 framework, and improve the documentation.

 Major changes:

 - HMM word alignment models, including a number of variants.  We have
   implemented the base model described in Och and Ney (CL, 2003), a class
   based variant also based on Och's work (though our implementation is based
   on the baseline system description in He, ACL/WMT-2007), as well as the
   variants described by Liang, Taskar and Klein (HLT-2006), including their
   symmetrization method, and He's (WMT-2007) lexicalized MAP (Bayesian) model.
   gen_phrase_tables also has a new PosteriorAligner based on Liang et al
   (HLT-2006).

   The HMM word alignment models are trained using train_ibm, they can be used
   for word alignment directly via align-words, and gen_phrase_tables can use
   them to generate phrase tables.  In our current state of the art, we
   typically perform word alignment using IBM2 and HMM models separately, then
   use the resulting phrase tables with cow.sh for maximum BLEU training,
   either separately or merged together into one table.

 - Added a generic HMM toolkit, used by the HMM word alignment models,
   supporting state or arc emitting HMMs, and implementing the Viterbi and
   Baum-Welch algorithms.  (Optimized for densely connected HMMs.)

 - Parallelized training of IBM1/2/HMM word alignment models via the new cat.sh
   script, and a binary format for TTables and all intermediate count files,
   for fast reading and writing of these model and count files.

 - An experimental framework is now included with PORTAGEshared, as a potential
   starting point for your experiments.  Besides demonstrating how to use
   PORTAGEshared, this framework embeds choices of options which we think are a
   reasonable starting point.

   Previously, the only full usage examples we provided were not suitable for
   this use.  The toy example was designed to run fast at all cost, regardless
   of the quality fo the output, while the small-vocabulary regression test
   suite was mostly intended to exercise the code.  The new framework is
   specifically designed to be both a tutorial and a reasonable starting point.
   Of course, you will still need to experiment in order to optimize
   performance for your setting.

   Even if you used PORTAGEshared before, we recommend you read
   framework-toy.pdf in the framework directory, as it includes a full
   description of how to use PORTAGEshared, including important features which
   are not all highlighted elsewhere.  If you have built your own experimental
   framework, you may find useful suggestions when following the toy example
   described in this document.

 - We've now included our truecasing module, which no longer requires external
   software at truecasing time.  The Perl script truecase.pl performs
   truecasing using canoe.  The program compile_truecase_map compiles the
   truecase map for truecase.pl.  Training the Language Model itself requires
   an external language modelling toolkit, e.g., SRILM (if your licensing
   requirements permit it) or IRSTLM.

 - Many improvements to run-parallel.sh, useful if you're working on a cluster:
   - uses Perl sockets instead of netcat, resulting in a significant reduction
     of overhead, from seconds down to hundredths of seconds per job, and one
     fewer dependency on external software;
   - now more stable, with more coherent behaviour in case of errors, which can
     be controled by the user via the new -on-error switch;
   - more thorough clean up at exit time or in case of errors;
   - all temporary files are hidden away in a workdir instead of polluting the
     directory run-parallel.sh is invoked from (most scripts using temporary
     files now do the same too);
   - new -c switch to run a single command via psub/qsub, acting as a blocking
     qsub for clusters that don't support blocking qsub - this is useful to
     have a Makefile run commands on a cluster via psub/qsub;
   - number of CPUs requested by the master job is propagated to the workers;
   - on clusters running Torque, take advantage of the job array feature to
     speed up and reduce the overhead of worker submission via psub/qsub.

 Minor changes:
 - User configuration is now centralized in src/Makefile.user-conf.
 - We've added some unit testing and a unit testing framework, using CxxTest,
   run automatically by doing "make test" in src/.
 - We've moved our legacy test programs into subdirectories of the source code,
   run automatically by doing "make test" in src/.
 - Make the code compile with g++ 4.3 without any warnings.
 - Streamlined c++ includes, in part to speed up compilation.
 - Use tr1::unordered_map instead of the soon to be deprecated
   __gnu_ext::hash_map.
 - Various code refactorings to make maintenance and documentation easier,
   including improvements to the compilation mechanism, removal of doxygen
   errors, and many more small details.
 - New section in the documentation with the usage info from all programs.
 - A few programs now have a -final-cleanup switch: for efficiency, we often
   don't delete models just before exiting, since the OS does so immediately
   after exiting.  In programs that support it, the -final-cleanup switch
   delete all models; useful for memory leak detection and other debugging.
 - Removed obsolete champollion.breakparts.pl, unsplit-sentence.pl,
   merge_ttables, maxphrase.pl, CalculateHypothesisProb.pl, and
   find_sentence_phrases.
 - Utils modules (src/utils):
   - New utf8 casemapping functionality (requires ICU)
   - Program utf8_filter performs strict validation of utf8 input.
   - diff-round.pl now supports compressed files automatically, and has new
     -sort, -q and -min options.
   - New template class BiVector models a vector with positive and negatives
     indices.
   - Fixed memory leak in short array allocator ArrayMemPool.
   - Added support for .lzma files in all C++ programs, via MagicStreams.
 - Preprocessing module (src/preprocessing):
   - Significantly improved the tokenizing and detokenizing for French;
     slightly improved for English as well.  Better lists of abbreviations in
     both languages.  Smart quotes and other characters from the cp-1252
     repertoire are now recognized, and optionally replaced by the closest
     iso-8859-1 characters.
   - New udetokenize.pl script performs detokenization on French and English
     text encoded in utf8.
 - Language Modelling module (src/lm):
   - New caching mechanism for LM queries, intended for use with expensive LM
     classes.  Currently only enabled for LMMix (Dynamic LM mixture model).
   - Minor improvements to ngram-count-big.sh: reports errors more carefully and
     removes the merge tree since we noticed a single multi-way merge is faster.
   - Refactored the LM classes to make adding new ones easier; added a new
     Factory Method creator object for each class.
   - New LM class: LMDynMap.  Used for dynamic mapping of case or numbers.
   - Tally statistics over LM queries, used by canoe in particular.
   - Renamed lmtext2binlm to the more precise name arpalm2binlm.
   - New script lm_sort_filter.sh sorts an ARPA-format LM in a way that
     typically increases its compression ratio with gzip.
   - New script lm-order.pl determines the order of an ARPA-format LM file.
 - Translation Modelling and Word Alignment module (src/tm):
   - New -prune1 option to gen_phrase_tables and joint2cond_phrase_tables
     prunes long tails before calculating probabilities.  Especially useful for
     phrase tables trained on noisy corpora.
   - New program word_align_tool allows manipulation and conversion of word
     alignment files.
   - Support for more word alignment formats via a generic module,
     word_align_io, which is easily extensible to support further formats.
   - Use smoothing for OOVs consistently in all alignment model queries.
   - New file handling-unaligned-words.txt explains how unaligned words are
     handled in the phrase extraction process.
   - New program eval_word_alignment calculates F-measure with respect to a
     reference alignment, as suggested by Fraser and Marcu (CL, Sept 2008).
   - New merge_counts program does fast merging of counts; used by
     gen-jpt-parallel.sh to remove large memory requirement at the end.
 - Eval module (src/eval):
   - Refactored PER, WER and BLEU calculations to support PER and WER
     optimisation in rescore_train.
   - bleucompare can now perform PER or WER calculations instead.
   - Support for NIST style BLEU computation by setting the environment
     variable PORTAGE_NIST_STYLE_BLEU (has different brevity penalty
     definition).
 - Decoding module (src/canoe):
   - canoe-parallel.sh now supports the use of load balancing in -append mode.
   - New preprocessing script canoe-escapes.pl adds or removes escapes expected
     by canoe as needed.
   - Bug fix in -soft-limit mode for filter_models keeps phrases pairs that
     were incorrectly deleted before.  This sometimes results in higher memory
     requirements in canoe, which can be addressed via gen_phrase_tables's new
     -prune1 switch.
   - New filter_models options: -ttable-limit overrides the limit in the
     canoe.ini file, -no-per-sent disables per-sentence LM filtering, using
     the less effective global-vocabulary LM filtering instead.
   - New rule decoder feature allows one to set weights for canoe markups via
     -rule-weigths, and tune them in cow.sh, instead of using hard-coded
     weights.  Supports multiple classes of rules with their separate weights.
   - Robustness fix: canoe now accepts non-finite numbers in phrase tables,
     issuing a warning and treating them as if they had been 0.
 - Rescoring module (src/rescoring):
   - New rescoring features: RatioFF, HMMTgtGivenSrc, HMMSrcGivenTgt,
     HMMVitTgtGivenSrc, HMMVitSrcGivenTgt, WerPostedit, PerPostedit,
     BleuPostedit, BackwardLM.
   - Generic rescoring feature SCRIPT invokes any script of your choice, for
     easy creation and prototyping of new features, as well as integration
     of features not part of PORTAGEshared.
   - New program uniq_nbest removes duplicates in an n-best list.
   - cow.sh and rescore_train now optionally optimize WER or PER instead of
     BLEU.
   - New micro tuning mode for cow.sh looks for per-sentence optimal weigths
     for a few iterations before looking for globally optimal weights.
   - Other new cow.sh options: -rescore-options, -no-lb, -s.
   - rescore_train -l saves a log of Powell runs for tracing the optimisation
     process; -rf randomizes the order in which features are considered by
     Powell's algorithm.
   - New script cowpie.py extracts useful statistics from a cow.sh log.
   - rescore_translate now supports Minimum Bayes Risk rescoring.
   - Rat.sh now uses hard phrase table filtering before translation, thus
     reducing memory requirements in canoe (can be disabled with -no-filt).
   - New rat.sh options: -rescore-opts, -per, -wer, -dep, -no-filt.
   - gen-features-parallel.pl converted to Perl (from bash) and made more
     stable.


v1.2 28.01.2008
 This is a significant update to PORTAGEshared, incorporating most of the
 changes we have made to Portage since the initial release of PORTAGEshared.

 Major changes:

 - Soft TM filtering: joint filtering of several phrase tables in such a way
   that, no matter what weights are used, the top L hypotheses will have been
   kept, i.e., discards entries that can never make it to the top L, under any
   set of non-negative weights.  Also described in Badr et al (2007).
   - Used by cow.sh when the -filt option is specified (recommended).

 - LM filtering based on per-sentence-vocabulary, as described in Badr et al
   (2007) (see the annotated bibliography in the user manual for all paper
   references).  In short, keeps an n-gram only if all the words it contains
   can occur together in the translation of at least one source sentence.
   Typical LM filtering uses one global vocabulary; this technique efficiently
   keeps track of a separate vocabulary for each input sentence to translate.
   In decoding, this approach can save some 25% of the memory required for
   large LMs, or as much as 50% when combined with soft TM filtering.  In
   lm_eval and the LM rescoring function, significantly higher savings are
   possible.  Works with both the text (ARPA) and our BinLM file formats.
   - Automatically used by canoe while loading language models;
   - automatically used by the rescoring module for the NgramFF feature;
   - used by lm_eval when the -per-sent-limit option is specified.

 - Implemented Huang and Chiang's (2007) cube pruning algorithm.  Can yield an
   order-of-magnitude speed up in decoding in most circumstances.  Requires
   careful re-tuning of some decoding parameters, however, especially S (stack
   size) since its meaning is not the same as with regular decoding.  Run canoe
   -h for details on enabling cube pruning.

 - New module implementing George Foster's LM and TM adaptation work, with
   integration of the resulting mixture models in the decoder - details in
   Foster and Kuhn (2007).

 - Optimization of various programs throughout the Portage suite, including
   canoe.

 - Significantly optimized string splitting routines (src/utils/str_utils.h)
   and consequently the loading of many types of input and data files.
   In particular, the new Voc::addConverter functor directly converts a
   sentence or a phrase from a string to a vector<Uint> with no intermediate
   storage, yield a noticeable speed up in several programs.

 - Implemented new language model heuristics for decoding, including
   "incremental", the default in several other MT systems, and now also the
   default in PORTAGEshared.

 - Monotonic decoding with phrase swaps can now be done using canoe's
   "-distortion-limit 0 -dist-phrase-swap" combination of options, optionally
   using the new PhraseDisplacement distortion model instead of, or in
   combination with, the standard distortion penalty (WordDisplacement).

 - New IBM1Forward decoder feature.

 - The main rescoring script, rat.sh, was overhauled to be easier to use.  The
   model is now specified with the same syntax as for rescore_train (documented
   in rescore_train -H): rat.sh transparently handles generating the features,
   managing temporary files (now all tucked away in a working sub-directory)
   and giving rescore_train an appropriately transformed model file.  See
   rat.sh -h for details and test-suite/regress-small-voc/28_rat_train.pl for
   an example rescoring model in this simplified syntax.

 - New rescoring features (run rescore_train -H for the full list):
   - IBM1DocTgtGivenSrc calculates p(tgt-sent|src-doc), using a file of docids
     to determine what parts of the source file constitute documents (the
     docids file should have one line for each line in the source text,
     containing an ID in any format (no whitespace allowed); lines with
     identical IDs are considered to come from the same document).
   - nbest*Post* - posterior probability features for confidence estimation
     rescoring - see Ueffing and Ney (2005), Zens and Ney (2006).  These papers
     not included in our annotated bibliography provide even more background
     and depth:
      - Blatz et al. (2003).  Confidence Estimation for Machine Translation.
        JHU/CLSP Summer Workshop.
      - Ueffing (2006).  Word Confidence Measures for Machine Translation.
        Ph.D. thesis.
   - Consensus and ConsensusWin - WER-based consensus over N-best list (very
     expensive features - not recommended for general use) - features based on
     Mangu et al (1999).
   - BLEUrisk - Minimum Bayes Risk using BLEU loss function - see Kumar and
     Byrne (2004).
   - ParMismatch and QuotMismatch - count mismatched parentheses and quotes.
   - CacheLM - cache LM over docs defined in docid files (see above) - see
     Kuhn and De Mori (1990-2).

 - Overhauled the regress-small-voc test suite:
   - exercices more aspects of the code;
   - includes two top-level scripts, one to run a minimal end-to-end suite, and
     a second one that also runs various extensions;
   - renumbered scripts so that they can be run in numerical sequence, as was
     originally the intention.

 Major bug fixes:
 - When canoe was used with -weight-f (weights on the forward translation
   models), the forward score did not get added to the total score, so that the
   forward score was in fact ignored during search.  Any experiments using
   -weight-f should be redone, as the result might be drastically different
   now.  Our own such experiments now have better results.

 Minor changes:
 - The new script process-memory-usage.pl monitors resource usage by a job,
   especially in a cluster setting.
 - Made the code compile mostly cleanly with g++ 4.2.0.
 - Formatting changes in many files to improve consistency: prefer 3-character
   intentation, avoid the tab character, streamline documentation, etc.
 - New utf8-adaptation of our English and French tokenizer script.
 - canoe:
   - added markup syntax documentation to the -help message;
   - new -check-input-only option to validate an input file, not decode;
   - improved error messages when invalid markup is encountered;
   - improved error recovery in -tolerate-markup-errors mode;
   - modified the markup language rules to "do what the user probably meant" by
     default when possible: an unescaped <, > or \ is treated literally (with a
     warning) if it can't be interpreted as markup.
   - option -segmentation-args is replaced by -segmentation-model model#args;
   - compress nbest and ffvals files on the fly;
   - the canoe.ini file format now allows lists to be separated by whitespace
     or ':', and allows comments introduced by '#';
   - new -ttable-log-zero option offers crude control of phrase table smoothing;
   - new -options switch prints a compact list of all canoe options;
   - new decoder feature based on forward IBM1 probabilities;
   - -dist-limit-ext and -dist-phrase-swap modify the semantics of the
     distortion limit;
   - new -input option allows input from any file, not just stdin;
   - new -append mode concatenates nbest, ffvals and pal files on the fly;
   - when using -random-weights (for cow.sh -mad), a specific distribution can
     be specified for each feature.
 - phrase_tm_align overhaul:
   - now reads a canoe.ini file and combines multiple models as canoe does,
     instead of accepting only one phrase table;
   - prints ffvals for all models in the canoe.ini file, even those that have
     no impact on alignment (e.g., lm).
 - canoe-parallel.sh:
   - new -lb option performs load balancing;
   - new -resume option attempts to recover a job that died previously, keeping
     the parts of the job that had been successful;
   - support for all the new canoe options.
 - filter_models and find_sentence_phrases:
   - use the same InputParser class as canoe for consistent handling of markup.
 - removed the broken markup handling from train_ibm, align-words and run_ibm:
   they now only accept tokenized, line-aligned plain text, as one would expect.
 - new program join_phrasetables generates a multi-prob (i.e., multi-column)
   phrase table from individual tables in the older single-column format.
 - normalized file names in src/rescoring: C++ code files are .cc, not .cpp,
   C++ template code files are -cc.h, not .cpp.
 - renamed ibm1aaron.{h,cc} ibm1wtrans.{h,cc} and the classes it contains to
   reflect the name of the feature implemented.
 - IBM 1 and 2 models: changed the default smoothing value from 1e-50 to the
   more appropriate value 1e-07.
 - run_ibm now optionally shows the log prob of each individual sentence pair
   and/or the perplexity for each input file pair.
 - Friendlier error message when trying to open a .gz file that doesn't exit.
 - get_voc: new -s option sorts results by reverse counts.
 - monitor-process.sh: now supports a polling frequency in 10ths of a second.
 - run-parallel.sh:
   - removed the dependency on faucet, using sockets directly instead;
   - using sockets reduced the per job overhead from about one second to about
     10ms, making it feasible to distribute thousands of tiny jobs accross
     several hosts;
   - dynamic options let the user add or remove workers, and kill the whole job
     easily.
 - psub:
   - support for any number of CPUs per node, specification of job priority,
     specification of node properties and monitoring of memory usage;
   - cleaned up to remove options that were clearly NRC specific.
 - PTrie class (utils/trie.h):
   - read_binary() now supports a mapper that remaps the keys while loading;
   - PTrie::iterator now allows insertion and deletion during traversal;
   - now supports an internal node value on the root;
 - VocMap (utils/voc_map.h):
   - new add() operation to insert a new word in both local and global vocab;
   - new operator() so it can be used as a functor in a fast split() call;
   - new local index<->word conversion methods.
 - bestbleu: new -bi option to display the best translation index.
 - bleumain, bestbleu, and all modules using bleu:
   - smoothing value 0 means no smoothing,
   - new options -y and -u to control the size of the ngrams considered,
   - many code optimizations.
 - BinLMs now run about 10% faster
 - lm/lm_eval replaces lm/testlm and can now calculate document perplexity.
 - The LM classes now properly support open-vocabulary language models.  As an
   optimization, open-voc LMs are assumed to only contain a probability
   P(<UNK>), and no other events involving <UNK>.  The LM classes also support
   LMs assigning probabilities and back-off weights to other events involving
   <UNK>, at the cost of some speed, so no program in PORTAGEshared enables
   this feature.  Changes will be required to the code to use general
   open-vocabulary LMs.
 - ngram-count-big.sh, our alternative to SRILM's make-batch-counts and
   merge-batch-counts, can now take advantage of parallel computing resources.
 - tm/ibm.{h,cc}: cleaned up the inheritance structure between classes IBM1 and
   IBM2 and filled in some missing documentation.
 - tm/joint2cond_phrase_tables:
   - new -j switch for merging joint frequency phrase tables;
   - by default, assume lexical model is IBM2, not the rarely used IBM1.
 - added gen-jpt-parallel.sh: parallel joint frequency phrase table training;
   the output can the be used by joint2cond_phrase_tables to estimate probs, as
   gen_phrase_tables would have done in a single non-parallel step.
 - gen_phrase_tables and joint2cond_phrase_tables:
   - new phrase smoother IBM1Smoother explicitely does IBM1 lexical smoothing,
     while IBMSmoother does IBM2 lexical smoothing if the model provided is
     IBM2, or IBM1 smoothing otherwise;
   - new Indicator phrase smoother creates a constant column.
 - canoe, api/paddle_server and api/portage_api.{h,cc}: model files in a
   canoe.ini are now assumed to be relative to the path of that canoe.ini file.
 - canoe/decoder.cc and canoe/basicmodel.cc: the CanoeConfig object is passed
   deeper instead of long parameter lists.
 - cow.sh:
   - one of -nofloor (new option) or -floor is required since there does not
     exist an appropriate default for this option;
   - temporary files are now compressed on the fly, unless -Z is specified;
   - -filt now uses the Soft TM filtering describe in Major changes above; the
     old grep-like filtering mode is preserved (via -filt-no-ttable-limit), but
     its use is not recommended;
   - other new options: -e, -lb.
 - When modifying and testing a single program, say src/<subdir>/<progname>,
   one can quickly rebuild it and its dependencies (and nothing else) by
   running this command in src/:
      gmake OT=<progname> <subdir>_progs
 - rat.sh:
   - by default, the output is now MODEL.out, rather than overwriting the input;
 - gen-features-parallel.sh now supports horizontal parallelism: besides
   running features in different processes (on different machines, if using a
   cluster), expensive features are split in several small blocks for better
   load balancing among available machines.
 - rescore_train:
   - can now specify the distribution to use for randomly generating starting
     points in the search for an optimum (run rescore_train -h for details);
   - significantly optimized, runs some 40% faster than before;
   - when compiled with OpenMP (edit build/Makefile.incl to enable this), runs
     in multi-thread mode, gaining another near two-fold speed up;
   - new -e switch implements a stopping criterion based on the statistical
     expectation of finding a better local optimum, given the distribution of
     previous ones found;
   - other new switches: -win, -r, -s, -y, -u, -sm.
 - rescore_translate:
   - new options: -s, -c, -kout (run rescore_translate -h for details);
 - rescoring in general:
   - new -y and -u switches control n-gram length for BLEU calculations;
   - the nebulous VFileFF feature has been removed; a much cleaner solution to
     the problem it solved is now implemented in rat.sh itself;
   - sets of n-best lists no longer need to have the same number of candidate
     translations for each source sentence, a feature that makes cow.sh more
     efficient;
   - features can now depend on the target language vocabulary (e.g., NgramFF
     uses this for filtering, as mentioned in Major Changes above).

 Minor bug fixes:
 - rescoring/featurefunction.cc: on some systems, what appears like a compiler
   bug generates broken code with std::for_each, so we stopped using it.
 - IBM features in rescoring: did not correctly allow NULL alignments.
 - block_mem_pool.h and index_block_mem_pool.h: in some cases, the destructor
   was called twice on the same object.
 - voc.cc: fixed mismatched malloc/free new/delete use.
 - rescoring feature IBM1Deletion was available but not hooked in properly.


v1.1 01.04.2007
 - Added the BinLM format for fast loading of LMs, supported by all
   programs loading LMs, with utility lmtext2binlm to generate them.
 - Made ibm model 2 the default in gen_phrase_tables.
 - Added the VFileFF rescoring "function" to rescore_* / rat.sh /
   gen_feature_values; now used in regress-small-voc/28_rat_train.pl.
 - Added -o option to rat.sh, which no longer generates .w files.


v1.0 31.10.2006
   Initial release.
