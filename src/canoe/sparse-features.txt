Sparse Features in PortageII
----------------------------

Sparse features are decoder features that are not necessarily active on each
target phrase. When they are active, they have value 1; otherwise they have
value 0. For example, a sparse feature might fire if the current target phrase
contains the word "green". On any particular hypothesis, only a relatively
small number of sparse features will typically be active.  The implementation
takes advantage of this to reduce the space and time requirements of sparse
features, so very large sets can be used - on the order of 10k, which is
probably several orders of magnitude more than the number of dense features
than would be viable.

The remainder of this document is organized as follows:

1. Using Sparse Features
2. Another Way to Train Sparse Weights
3. Writing Sparse Features
4. Nuts and Bolts
5. Improvements

1. Using Sparse Features
------------------------

To use sparse features in PortageII, you first create a sparse model file
containing "event template" specifications. Each event template generates one
or more sparse features (later we'll get to why these are called event
templates and not feature templates, but for now you can think of them as
feature templates).  A sparse model file is like a canoe.ini file, and event
templates are specified using the same syntax as parameters in a canoe.ini. For
example:

   [PhrasePairAllBiLengthBins] 7 7

is a specification for the template "PhrasePairAllBiLengthBins" with arguments
"7 7". This generates 49 sparse features, one for every pair of source/target
phrase lengths up to a maximum of 7 in each language. To list available event
templates and their arguments, use "palminer -H" - that's a capital "H".

Once you have created a sparse model file, called for instance "model",
containing your desired templates, you compile it with the following command:

   palminer -pop -m model

(Why is the program to compile sparse models called "palminer"? We'll get to
that too.) The "-pop" option means to populate the model with all sparse
features that can be generated by each template, and initialize each with a
weight of 0. This command creates a set of files "model.*" which you can mostly
ignore, but it also writes out some logging information about the model that
can be useful, including the number of sparse features it contains. Keep in
mind that this is an upper bound on the number of features that will actually
fire on any given text, which for most models will be considerably lower.

One thing to be aware of when compiling sparse models is that data files you
provide as arguments to event templates are generally not copied into the
compiled model. They need to remain accessible during decoding, so don't delete
them or move them away.  (They also formerly needed to be specified using
absolute path names, but now, through the magic of auto-relativization, they
can have path names that are relative to the model file. If you don't know what
this means, don't worry - just do the natural thing and it will work.)

You typically won't have to worry about setting up data files yourself. The
script build-sparse-model.sh, integrated in Framelab and the PortageII
framework, knows about many different types of data file and will create them
automatically from parameters specified in the config file (in Framelab) or the
sparse-specs-* files (in the PortageII framework).  For example, the
"AlignedWordPair" template takes two data file arguments, for source and target
vocabularies. To create these automatically, using the 80 most frequent words
in the training corpus, you can put the following line in the config file:

   [AlignedWordPair] auto:svoc:$corp:80 auto:tvoc:$corp:80

This will create the two vocabulary files, give them sensible names, and write
a corresponding line to the resulting "model" file:

   [AlignedWordPair] sensible-src-voc-filename sensible-tgt-voc-filename

See the documentation in Framelab's models/sparse/default/config for more
details.

To recap, a sparse model consists of a model file, which we're calling "model",
a set of associated compiled files, "model.*" in the same directory, and,
typically, a set of external data files specified in "model". To use this in
the decoder, you put the following line in the canoe.ini:

   [sparse-model] path/to/modelfile

From canoe's perspective, a sparse model is a single feature in the log-linear
model; all of the individual sparse features and their weights are hidden
inside. However, you can still peek at these using "canoe
-describe-model-only". By default, this produces a compact listing with one
entry per template, but with "-v 2" or higher, it will give detailed info about
all sparse features.

Like other decoder features, weights on sparse features get set during tuning.
Unlike other decoder features, you need to give the -r flag to tune.py to make
this happen (if you're using Framelab, -r will normally be set automatically if
you include a sparse model). The -r option does two things:

1) It runs canoe with the -sfvals flag, which causes feature vectors in nbest
lists and lattices to be written in sparse format, with individual sparse
features "unpacked" so MIRA et al can get at them. Sparse format just means
that features are written as index/value pairs, omitted when their value is 0,
rather than as position-specific entries in a complete list. This applies to
dense features too, but most of those won't be 0 most of the time.

2) It causes weights on individual sparse features to be included in the weight
vector that gets read from and written to the canoe.ini file used for decoding
(via configtool {set,get}-all-wts commands). Sparse weights aren't explicitly
part of a canoe.ini like normal weights are, but you can think of them as being
implicitly there. On the first tuning iteration, they are read from a weights
file in the sparse model directory ("model.wts.gz" in the example above), which
usually contains 0s. After that, they are read to and written from a local
weights file that encodes the path of the sparse model. In Framelab, for
instance, this file might be named "r.._.._sparse_baseline_model.wts.gz", which
encodes the path "../../sparse/baseline/model".

Warning: if canoe can't find a local sparse weights file with the proper name,
it will use weights from "model.wts.gz" in the sparse model directory, which is
probably not what you want, since it contains 0s by default. When copying local
weights files from tuning runs, or when moving or renaming sparse model
directories, make sure the tuned weights get picked up properly. You can check
this by looking at canoe's log files and searching for a line saying "local
SparseModel weights file found...".

What if you tune a sparse model without -r? The sparse model is treated as a
single feature whose value is the sum of the weights on active sparse features
it contains. A weight is learned for this global feature, while the weights on
individual sparse features are fixed at whatever values they had to begin
with. This can be useful to "calibrate" an existing sparse model for new dev
material that might be too small to reliably learn individual sparse weights
from, or for combining several existing sparse models trained on other data
(any number of sparse models can be used in canoe). It is obviously not useful
with a sparse model in which the individual weights are 0 to begin with.

By the way, the global sparse feature whose weight gets tuned when not using -r
is always part of any decoder model that contains sparse features. In canoe's
feature listing, it shows up as "SparseModel:/path/to/model/file". When -r IS
being used, its value gets set to 0 in the nbest lists or lattices that the
tuning algorithms work with (so it plays no role), and its weight is always 1
in the model read in by canoe (so the individual sparse feature weights get
used as-is).

2. Another Way to Train Sparse Weights
--------------------------------------

This section describes an experimental capability that hasn't been used in any
published or competition system, so you can skip it if you're only interested
in typical sparse feature usage. The two last paragraphs contain a few items
that you might want to know about, however.

As its name implies, the palminer program does more than just create empty
sparse models from feature specifications. It was originally designed to train
sparse models in online fashion from large, phrase-aligned ("pal") corpora. It
does this by stepping through phrase pairs like the decoder does and, at each
position, comparing the model's choice of next phrase pair with the phrase pair
from the reference alignment, then updating weights using the perceptron
algorithm. There are different options for the model that is used to choose the
next phrase pair, including the regular decoder model without sparse features,
the regular model including whatever sparse features have been learned so far,
or no model at all. If no model is used, the algorithm amounts to counting
features that apply to the reference phrase pairs, which might be useful as
suggested below. There are some subtleties associated with the exact definition
of reference and model phrase pairs that are described in the "palminer -h"
documentation.

But there's more. In addition to the demanding task of large-scale
discriminative learning for SMT, palminer makes life even more difficult for
itself by optionally trying to learn feature conjunctions online using an ad
hoc algorithm. This is where the distinction between events (generated by
user-specified event templates) and features comes in. Events are predicates
that describe any part of a partial hypothesis, such as that the current target
phrase contains the word "green", or the previous source phrase contains the
word "ciel". Features are conjunctions of events in which at least one event
must pertain to the current target phrase. So, in this example, "green" could
be a feature, as could "green" AND "ciel", but not "ciel" on its own.

To learn conjunctions, palminer figures out what features and events are active
in the current context. Single events that apply to the target phrase are
automatically promoted to features. Then, if the number of active features is
sufficiently low, it creates new features by conjoining each current feature
that has been observed often enough in the past with each active event (where
"sufficiently low" and "often enough" are specified by parameters). This
operation is fairly complex, since it has to avoid conjoining events with
features that already subsume them. To speed up learning, the
"palminer-parallel.sh" script can be used to shard the data, run palminer on
each shard, and then put the results back together with the
"merge_sparsemodels" program.

Note that "merge_sparsemodels" is useful for things other than sharded
training: it can dump the contents of a sparse model, or prune it to a given
number of desired features.

There is a set of event templates - the ones prior to "TgtNgramCountBin" in the
"palminer -H" listing - that can only be used in palminer's learning mode.
These templates, which capture words at various positions in source or target
phrases, are designed to acquire their vocabulary during training, rather than
having it be specified through a data file argument. They will therefore not
produce any features if you use them to populate a model via "palminer -pop".
Since these templates, and feature conjunctions generated from them, are still
potentially useful, you might want to consider running palminer training on a
dev set in order to learn high-frequency (or highly-discriminative)
conjunctions. The resulting model could then be tuned as usual, possibly along
with another sparse model produced using the usual "palminer -pop" command. It
would also be interesting to be able to manually specify event conjunctions,
but there is no interface for this (yet).

3. Writing Sparse Features
--------------------------

Writing a new sparse feature (more precisely, an event template) is similar to
writing a new decoder feature, except that you follow the instructions under
EventTemplate in "sparsemodel.h". The main work is in providing instantiations
for EventTemplate virtual functions which the SparseModel class uses to satisfy
the requirements of the decoder feature interface. When doing so you should
make use of various SparseModel resources that can be shared among different
event templates, such as vocabularies, ngram counts, and class maps.

The main EventTemplate virtual function is addEvents(), which generates the
events that are active in the current context, in the form of integer indexes
appended to a global list. Each template has its own index space, so there is
no need to worry about collisions with other templates. A template can generate
multiple events, but events can't repeat in the list. This is a software
constraint, arising from the mechanism to create conjunctions, that can produce
inconsistencies, as illustrated by the following example:

   "green grass is green" 
   "green grass ||| is green"

A feature that indicates the presence of "green" can only fire once on the
first target phrase, but will fire twice if it is broken up into the two
phrases shown.

The EventTemplate virtual to handle recombination is addContextEvents(). The
mechanism for recombination in SparseModel is designed for settings in which
features will mostly be conjunctions of events that characterize only the
current context (not including the final phrase pair) with events that
characterize only the final phrase pair.  Unfortunately, features like that are
not normally used, because we don't use palminer's mechanism for creating
conjunctions. All features we normally use correspond to a single event
template that depends on the current phrase pair. In cases where the event
template depends ONLY on the phrase pair, this is fine, because such templates
will never affect recombination. However, event templates that depend on
additional context should in principle instantiate addContextEvents() so as to
add any events that characterize all possible current phrases for a given
context.  None of the templates that depend on additional context currently
satisfy this onerous requirement. For the record, these are TgtNgramCountBin,
all Dist* templates, and all Lm* templates. Provided the context used by the LM
is longer than that required by TgtNgramCountBin and the Lm* templates, and the
right kind of lexicalized reordering model is used for the Dist* templates
(hierarchical is required for some), this won't affect recombination beyond
what is already provided for by the LM or the DM. However, there is no
mechanism to check these conditions.

Notice that there are no explicit functions in the EventTemplate interface to
handle future scores. The only future score component computed by SparseModel
is precomputeFutureScore(), which is performed only if the #pfs flag was
appended to the model name by the user. This computation involves only
templates that are local to the current phrase pair, as declared by the
isPure() virtual function.

The final non-trivial EventTemplate virtual is remapEvent(), which is called by
SparseModel when it has to replace the current shared vocabulary (word <->
index mapping) for various purposes. The default instantiation is sufficient
for templates in which vocabulary indexes correspond 1-1 with event indexes,
but templates that make use of the vocabulary for other purposes might need to
take special action.

4. Nuts and Bolts
-----------------

For posterity, here is the algorithm for constructing a SparseModel Boolean
feature vector in some context (PartialTranslation, including current phrase
pair under evaluation):

a. Initialize the active_features map (feature -> event count) and
   active_events list.

b. Poll all active EventTemplates. Each yields 0 or more events, in the form of
   (template_id, local_id) pairs, where template_id identifies the generating
   EventTemplate, and local_id is specific to that EventTemplate. Store results
   in the active_events list.

c. For each active event (template_id, local_id) pair:

   i. Look up the event in global events map. This maps an event to all
   features that it participates in (atomic or conjunctions).

   ii. If the event isn't found, and if applies to the current phrase (as
   opposed to just the context), and if we're learning new features, create a
   new atomic feature for the event. Add an entry to the global events map, and
   an entry to the global features list (maps feature id -> set of events that
   constitute the feature).

   iii. For each feature triggered by the event (possibly none), increment its
   event count in the active_features list. 

d. Remove from the active features map all features whose event counts are less
   than their required event counts, as obtained from the global features list.
   For example, a conjunction of three events will be removed if it has an
   event count of only two.

e. If learning conjunctions, and if the current active_features set is not too
   big, then for each event in the active_events list, try to form a new
   conjunctive feature with each feature in the active_features list that has
   been active in the past sufficiently often. This succeeds if:

   i. The feature doesn't contain the event already, or if there is no
   subsumption relation in either direction between the new event and any of
   the events that constitute the feature.

   ii. The resulting set of events isn't already a feature. (Expensive to
   check. Currently we find the event that belongs to the smallest set of
   features, and explicitly compare each of their event sets to the current
   one. This is why event sets are kept sorted in the features list).

On success, add the new feature to the current global features list, and add a
pointer to the feature to the entry for each constituent event in the global
events map.

5. Improvements
---------------

Here is a list of things that could be improved in the implementation, in no
particular order:

1. Events should be allowed to fire multiple times in a single context. This is
likely to be a non-trivial change.

2. The current complex mechanism for mapping events to features is redundant
unless conjunctions are used. It should be profiled, and either made more
efficient or replaced if it turns out to be a bottleneck.

3. When they fire, sparse features produce 1's in log space, which violates the
assumption in future-score computations throughout canoe that 0 is the highest
score. The effect of this is unknown, and should be investigated.

4. Future score computations in SparseModel are rudimentary, as described in
"Writing Sparse Features", and are also turned off by default. This default
should be revisited, after experiments. It might also be possible to implement
better future scores for some templates.

5. The SparseModel mechanism for recombination is largely irrelevant for
features that don't include conjunctions, as discussed in "Writing Sparse
Features". Correct recombination generally depends on certain properties of the
global model, such as LM order. At a minimum these properties should be
checked; ideally the whole mechanism should be re-written.

6. A potentially interesting use of conjunctions is to be able to manually
specify them at the template level in a sparse model file. This would require
an appropriate syntax, such as "[Template1] arg1 & [Template2] arg2", but would
probably be relatively trivial to implement apart from that.
